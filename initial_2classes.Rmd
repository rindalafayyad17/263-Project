---
title: '263 Project: Redo with Two Classes'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(MASS)
library(gbm)
library(glmnet)
library(nnet)
library(cvms)
library(tibble)
library(table1)
library(fastshap)
library(caret)
library(caret)
library(ISLR)
library(gt)
library("shapper")
#install_shap()
```



```{r}

#Loading the dataset 

diabetes <- read.csv(file = "diabetes_012_health_indicators_BRFSS2015.csv", header = TRUE)
#View(diabetes)

# move anyone from class 0 into class 1
diabetes$Diabetes_012[diabetes$Diabetes_012 == 0] <- 1
colnames(diabetes)[1] <- "Diabetes_12"

diabetes$Diabetes_12 <- as.factor(diabetes$Diabetes_12)

# factor_variables <- c(1:4,6:15,18:22) #the columns that should be factors
# 
# #making the variables as factors
# for (i in factor_variables){
#   diabetes[,i] <- as.factor(diabetes[,i])
# }

```



Table 1:

```{r}

diabetes_table1 <- diabetes

diabetes_table1$Diabetes_12 <- factor(diabetes_table1$Diabetes_12, levels = c("1", "2"), labels = c("Non-Diabetic or Prediabetic", "Diabetic"))
label(diabetes_table1$Diabetes_12) <- "Diabetes Status"

diabetes_table1$HighBP <- factor(diabetes_table1$HighBP, levels = c("0","1"), labels = c("No High BP", "High BP"))
label(diabetes_table1$HighBP) <- "Blood Pressure"

diabetes_table1$HighChol <- factor(diabetes_table1$HighChol, levels = c("0", "1"), labels = c("No High Cholesterol", "High Cholesterol"))
label(diabetes_table1$HighChol) <- "Cholesterol Level"

diabetes_table1$CholCheck <- factor(diabetes_table1$CholCheck, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$CholCheck) <- "Cholesterol Check in past 5 Years"

diabetes_table1$Smoker <- factor(diabetes_table1$Smoker, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Smoker) <- "Smoked At Least 100 Cigs in Lifetime"

diabetes_table1$Stroke <- factor(diabetes_table1$Stroke, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Stroke) <- "Ever Told Had a Stroke"

diabetes_table1$HeartDiseaseorAttack <- factor(diabetes_table1$HeartDiseaseorAttack , levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$HeartDiseaseorAttack) <- "CHD or Myocardial Infarction"

diabetes_table1$PhysActivity <- factor(diabetes_table1$PhysActivity , levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$PhysActivity) <- "Physical Activity in Past 30 Days"

diabetes_table1$Fruits <- factor(diabetes_table1$Fruits, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Fruits) <- "Consume At Least 1 Fruit per Day"

diabetes_table1$Veggies <- factor(diabetes_table1$Veggies, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Veggies) <- "Consume At Least 1 Vegetable per Day"

diabetes_table1$HvyAlcoholConsump <- factor(diabetes_table1$HvyAlcoholConsump, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$HvyAlcoholConsump) <- "Heavy Drinker"

diabetes_table1$AnyHealthcare <- factor(diabetes_table1$AnyHealthcare, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$AnyHealthcare) <- "Have Any Kind of Health Care Coverage"

diabetes_table1$NoDocbcCost <- factor(diabetes_table1$NoDocbcCost, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$NoDocbcCost) <- "Could Not See a Doctor in Past 12 Months Due To Cost"

diabetes_table1$GenHlth <- factor(diabetes_table1$GenHlth, levels = c("1", "2", "3", "4", "5"), labels = c("Excellent", "Very Good", "Good", "Fair", "Poor"))
label(diabetes_table1$GenHlth) <- "General Health Status"

label(diabetes_table1$MentHlth) <- "Nb Days Mental Health Not Good in Past 30 Days"

label(diabetes_table1$PhysHlth) <- "Nb Days Physical Health Not Good in Past 30 Days"

diabetes_table1$DiffWalk <- factor(diabetes_table1$DiffWalk, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$DiffWalk) <- "Serious Difficulty Walking or Climbing Stairs"

diabetes_table1$Sex <- factor(diabetes_table1$Sex, levels = c("0", "1"), labels = c("Female", "Male"))
label(diabetes_table1$Sex) <- "Sex Assigned at Birth"

diabetes_table1$AgeBinary <- ifelse(diabetes_table1$Age < 9, "Young", "Old")
diabetes_table1$AgeBinary <- factor(diabetes_table1$AgeBinary, levels = c("Young", "Old"), labels = c("Less Than 60 Years Old", "At Least 60 Years Old"))
label(diabetes_table1$AgeBinary) <- "Age"

diabetes_table1$Education <- factor(diabetes_table1$Education, levels = c("1","2", "3", "4", "5", "6"), labels = c("Never Attended School", "Grades 1 Through 8 (Elementary)", "Grades 9 Through 11 (Some High School)", "Grade 12 or GED (High School Graduate)", "1 to 3 Years of College (Some College or Technical School)", " 4 or More Years of College (College Graduate)"))
label(diabetes_table1$Education) <- "Highest Education Level"

diabetes_table1$IncomeBinary <- ifelse(diabetes_table1$Income < 6, "Low", "High")
diabetes_table1$IncomeBinary <- factor(diabetes_table1$IncomeBinary, levels = c("Low", "High"), labels = c("At Most $35,000", "More Than $35,000"))
label(diabetes_table1$IncomeBinary) <- "Annual Income"


table1(~ Sex + AgeBinary + HighBP + HighChol + CholCheck + BMI + Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + Fruits + Veggies + HvyAlcoholConsump + AnyHealthcare + NoDocbcCost + GenHlth + MentHlth + PhysHlth + DiffWalk  + Education +  IncomeBinary | Diabetes_12, data = diabetes_table1, overall = "Total")


```

```{r}

set.seed(356) #Daniel's record

#splitting the dataset into train and test sets
n <- nrow(diabetes)
train_indices <- sample(1:n, size = 0.75*n, replace = F) #picking 3/4 of the dataset to be the training set and 25% to be the test set

train_set <- diabetes[train_indices, ] #training set
ytrain <- train_set$Diabetes_12

test_set <- diabetes[-train_indices, ] #test set
ytest <- test_set$Diabetes_12
```


<br>

<br>
```{r}
get_recall <- function(pred, target){
   TP = sum((pred==2) & (target==2))
   TN = sum((pred==1) & (target==1))
   FP = sum((pred==2) & (target==1))
   FN = sum((pred==1) & (target==2))
   recall = TP/(TP+FN)
   recall
}

get_precision <- function(pred, target){
   TP = sum((pred==2) & (target==2))
   TN = sum((pred==1) & (target==1))
   FP = sum((pred==2) & (target==1))
   FN = sum((pred==1) & (target==2))
   precision = TP/(TP+FP)
   precision
}

get_accuracy <- function(pred, target){
   TP = sum((pred==2) & (target==2))
   TN = sum((pred==1) & (target==1))
   FP = sum((pred==2) & (target==1))
   FN = sum((pred==1) & (target==2))
   accuracy = (TP+TN)/(TP+TN+FP+FN)
   accuracy
}
```


```{r}
evaluate_on_income <- function(pred, target, income) {
   df <- data.frame(pred, target, income)
   df$income = ifelse(df$income < 6, 0, 1)

   colnames(df) <- c('pred','target','income')
   list_metrices <- list()
   for (i in (0:1)){
      filtered_df = df[df$income==i, ]
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i+1]] = c(p, r, a, fraction)
   }
   list_metrices
}
```


```{r}
evaluate_on_education<- function(pred, target, education) {
   df <- data.frame(pred, target, education)
   df$education = ifelse(df$education < 5, 1, 2)
   colnames(df) <- c('pred','target','education')
   list_metrices <- list()
   for (i in (1:2)){
      filtered_df = df[df$education==i, ]
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i]] = c(p, r, a, fraction)
   }
   list_metrices
}
```

```{r}
evaluate_on_age<- function(pred, target, age) {
   df <- data.frame(pred, target, age)
   df$age = ifelse(df$age < 9, 1, 2)
   colnames(df) <- c('pred','target','age')
   list_metrices <- list()
   for (i in (1:2)){
      filtered_df = df[df$age==i, ]
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i]] = c(p, r, a, fraction)
   }
   list_metrices
}
```

```{r}
evaluate_on_binary_variables <- function(pred, target, variable_values, variable_name) {
   df <- data.frame(pred, target, variable_values)
   colnames(df) <- c('pred','target',variable_name)
   list_metrices <- list()
   for (i in (0:1)){
      filtered_df = df[df[,variable_name]==i, ]
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i+1]] = c(p, r, a, fraction)
   }
   list_metrices
}
```

1. LDA 

```{r}
#fitting LDA model using all the variables as predictors

lda_fit <- lda(Diabetes_12 ~ ., data = train_set, method = "mle")


cutoffs <- seq(0.1, 0.5, by = 0.05)
table_lda <- as.data.frame(matrix(nrow = 9, ncol = 5))
colnames(table_lda) <- c("Cutoff", "Accuracy", "Precision", "Recall", "Specificity")
table_lda[,1] <- cutoffs

for (i in 1:length(cutoffs)){
   
   #predictions using training set
   phat_train_lda <- predict(lda_fit, newdata = train_set, method = "plug-in")$posterior[,2]
   fhat_train_lda <- ifelse(phat_train_lda > cutoffs[i], 2, 1)
   train_error_lda <- mean(fhat_train_lda != ytrain) #training error
   #train_error_lda 
   
   #predictions using test set
   phat_test_lda <- predict(lda_fit, newdata = test_set, method = "plug-in")$posterior[,2]
   fhat_test_lda <- ifelse(phat_test_lda > cutoffs[i], 2, 1)
   test_error_lda <- mean(fhat_test_lda != ytest) #test error
   #test_error_lda 
   
   lda.cm <- confusionMatrix(as.factor(fhat_test_lda), ytest, positive = "2")
   lda.accuracy <- lda.cm$overall[1]
   lda.precision <- lda.cm$byClass[3]
   lda.recall <- lda.cm$byClass[1]
   lda.spec <- lda.cm$byClass[2]
   
   table_lda[i,2] <- lda.accuracy
   table_lda[i,3] <- lda.precision
   table_lda[i,4] <- lda.recall 
   table_lda[i,5] <- lda.spec

   
}

table_lda[,2:5] <- round(table_lda[,2:5], digits = 2)
#table_lda
```


```{r}
# Get precision, recall, accuracy and fraction for Sex, Healthcare Coverage, Income, Education and Age
evaluate_on_binary_variables(fhat_test_lda, ytest, test_set$Sex, "Sex")
evaluate_on_binary_variables(fhat_test_lda, ytest, test_set$AnyHealthcare, "AnyHealthcare")
evaluate_on_income(fhat_test_lda, ytest, test_set$Income)
evaluate_on_education(fhat_test_lda, ytest, test_set$Education)
evaluate_on_age(fhat_test_lda, ytest, test_set$Age)
```
```{r}
pred_fun_lda <- function(model_fit, newdata) {
  predict(model_fit, newdata, n.trees = 500, type = "response")$posterior[,2]
}

shap <- explain(lda_fit, X = test_set[, -1], pred_wrapper = pred_fun_lda, nsim = 20)
library(ggplot2)
autoplot(shap)
```



```{r}
# In the middle of each tile, we have the normalized count (overall percentage) and, beneath it, the count.
# At the bottom, we have the column percentage.
# At the right side of each tile, we have the row percentage. 


# LDA training 2x2 confusion matrix -- actual value, predicted value

train.matrix.lda <- as_tibble(table(ytrain, fhat_train_lda))

plot_confusion_matrix(train.matrix.lda, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_lda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# LDA Test 2x2 confusion matrix -- actual value, predicted value

test.matrix.lda <- as_tibble(table(ytest, fhat_test_lda))

plot_confusion_matrix(test.matrix.lda, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_lda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )
# 
# lda.cm <- confusionMatrix(fhat_test_lda, ytest, positive = "2")
# lda.accuracy <- lda.cm$overall[1]
# lda.precision <- lda.cm$byClass[3]
# lda.recall <- lda.cm$byClass[1]
# lda.spec <- lda.cm$byClass[2]
```






<br>

<br>


2. QDA

```{r}
#fitting QDA model using all the variables as predictors

qda_fit <- qda(Diabetes_12 ~ ., data = train_set, method = "mle")

table_qda <- as.data.frame(matrix(nrow = 9, ncol = 5))
colnames(table_qda) <- c("Cutoff", "Accuracy", "Precision", "Recall", "Specificity")
table_qda[,1] <- cutoffs

for (i in 1:length(cutoffs)){
   
   #predictions using training set
   phat_train_qda <- predict(qda_fit, newdata = train_set, method = "plug-in")$posterior[,2]
   fhat_train_qda <- ifelse(phat_train_qda > cutoffs[i], 2, 1)
   train_error_qda <- mean(fhat_train_qda != ytrain) #training error
   #train_error_lda 
   
   #predictions using test set
   phat_test_qda <- predict(qda_fit, newdata = test_set, method = "plug-in")$posterior[,2]
   fhat_test_qda <- ifelse(phat_test_qda > cutoffs[i], 2, 1)
   test_error_qda <- mean(fhat_test_qda != ytest) #test error
   #test_error_lda 
   
   qda.cm <- confusionMatrix(as.factor(fhat_test_qda), ytest, positive = "2")
   qda.accuracy <- qda.cm$overall[1]
   qda.precision <- qda.cm$byClass[3]
   qda.recall <- qda.cm$byClass[1]
   qda.spec <- qda.cm$byClass[2]
   
   table_qda[i,2] <- qda.accuracy
   table_qda[i,3] <- qda.precision
   table_qda[i,4] <- qda.recall 
   table_qda[i,5] <- qda.spec

   
}

table_qda[,2:5] <- round(table_qda[,2:5], digits = 2)
#table_qda

```

```{r}
evaluate_on_binary_variables(fhat_test_qda, ytest, test_set$Sex, "Sex")
evaluate_on_binary_variables(fhat_test_qda, ytest, test_set$AnyHealthcare, "AnyHealthcare")
evaluate_on_income(fhat_test_qda, ytest, test_set$Income)
evaluate_on_education(fhat_test_qda, ytest, test_set$Education)
evaluate_on_age(fhat_test_qda, ytest, test_set$Age)
```

```{r}
pred_fun_lda <- function(model_fit, newdata) {
  predict(model_fit, newdata, n.trees = 500, type = "response")$posterior[,2]
}

shap <- explain(qda_fit, X = test_set[, -1], pred_wrapper = pred_fun_lda, nsim = 20)
library(ggplot2)
autoplot(shap)
```

```{r}
# QDA train 2x2 confusion matrix

train.matrix.qda <- as_tibble(table(ytrain, fhat_train_qda))

plot_confusion_matrix(train.matrix.qda, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_qda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# QDA test 2x2 confusion matrix

test.matrix.qda <- as_tibble(table(ytest, fhat_test_qda))

plot_confusion_matrix(test.matrix.qda, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_qda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )

# qda.cm <- confusionMatrix(fhat_test_qda, ytest, positive = "2")
# qda.accuracy <- qda.cm$overall[1]
# qda.precision <- qda.cm$byClass[3]
# qda.recall <- qda.cm$byClass[1]
# qda.spec <- qda.cm$byClass[2]
```

interesting that QDA is correctly predicting more people who actually have diabetes (and is also falsely predicting too many people to have diabetes). this might actually be more useful


<br>

<br>


4. Logistic

```{r}
#Fitting a multinomial model with all variables as predictors.
logit_fit <- glm(Diabetes_12 ~ . , family = binomial(), data = train_set)


table_logit <- as.data.frame(matrix(nrow = 9, ncol = 5))
colnames(table_logit) <- c("Cutoff", "Accuracy", "Precision", "Recall", "Specificity")
table_logit[,1] <- cutoffs

for (i in 1:length(cutoffs)){
   
   #training error
   phat_train_logit <- predict(logit_fit, train_set, type = "response")
   fhat_train_logit <- ifelse(phat_train_logit > cutoffs[i], 2, 1)
   train_error_logit <- mean(fhat_train_logit != ytrain)
   #train_error_logit 
   
   #test error
   phat_test_logit <- predict(logit_fit, test_set)
   fhat_test_logit <- ifelse(phat_test_logit > cutoffs[i], 2, 1)
   test_error_logit <- mean(fhat_test_logit != ytest)
   #test_error_logit
   
   logit.cm <- confusionMatrix(as.factor(fhat_test_logit), ytest, positive = "2")
   logit.accuracy <- logit.cm$overall[1]
   logit.precision <- logit.cm$byClass[3]
   logit.recall <- logit.cm$byClass[1]
   logit.spec <- logit.cm$byClass[2]
   
   table_logit[i,2] <- logit.accuracy
   table_logit[i,3] <- logit.precision
   table_logit[i,4] <- logit.recall 
   table_logit[i,5] <- logit.spec

   
}

table_logit[,2:5] <- round(table_logit[,2:5], digits = 2)
#table_logit


```


```{r}
evaluate_on_binary_variables(fhat_test_logit, ytest, test_set$Sex, "Sex")
evaluate_on_binary_variables(fhat_test_logit, ytest, test_set$AnyHealthcare, "AnyHealthcare")
evaluate_on_income(fhat_test_logit, ytest, test_set$Income)
evaluate_on_education(fhat_test_logit, ytest, test_set$Education)
evaluate_on_age(fhat_test_logit, ytest, test_set$Age)
```

can use a different cutoff for logistic!
```{r}
lr_importance = (varImp(logit_fit, scale = FALSE))
lr_importance <- cbind(newColName = rownames(lr_importance), lr_importance)
names(lr_importance) <- c("Feature", "Coefficient")
lr_importance <- lr_importance[order(lr_importance$Coefficient, decreasing = TRUE),]
ggplot(lr_importance, aes(x = Coefficient, y = reorder(Feature, Coefficient)))+geom_col()+xlab("Maginitude of Coefficient") + ylab("Feature")
```

```{r}
# logistic train 2x2 confusion matrix

train.matrix.logit <- as_tibble(table(ytrain, fhat_train_logit))

plot_confusion_matrix(train.matrix.logit, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_logit",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# logistic test 2x2 confusion matrix

test.matrix.logit <- as_tibble(table(ytest, fhat_test_logit))

plot_confusion_matrix(test.matrix.logit, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_logit",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )
# 
# logit.cm <- confusionMatrix(as.factor(fhat_test_logit), ytest, positive = "2")
# logit.accuracy <- logit.cm$overall[1]
# logit.precision <- logit.cm$byClass[3]
# logit.recall <- logit.cm$byClass[1]
# logit.spec <- logit.cm$byClass[2]
```



Don't run this it will take 1 hour (literally). I ran it and the backward selection only removed the fruits and veggies variables. So I ran another multinomial model excluding fruits and veggies.
```{r}
#multinomial using backward selection to pick the best variables

#start <- multinom(Diabetes_012 ~ ., data = train_set)
#backward_multinom <- step(start, direction = "backward")
```

```{r}

#logistic model excluding fruits and veggies variable
logit_fit2 <- glm(Diabetes_12 ~ HighBP + HighChol + CholCheck + BMI + Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + HvyAlcoholConsump + AnyHealthcare + NoDocbcCost + GenHlth + MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + Income, family = binomial(), data = train_set )

#training error
phat_train_logit2 <- predict(logit_fit2, train_set)
fhat_train_logit2 <- ifelse(phat_train_logit2 > 0.5, 2, 1)
train_error_logit2 <- mean(fhat_train_logit2 != ytrain)
train_error_logit2 

#test error
phat_test_logit2 <- predict(logit_fit2, test_set)
fhat_test_logit2 <- ifelse(phat_test_logit2 > 0.5, 2, 1)
test_error_logit2 <- mean(fhat_test_logit2 != ytest)
test_error_logit2
```


```{r}
# logistic2 train 2x2 confusion matrix

train.matrix.logit2 <- as_tibble(table(ytrain, fhat_train_logit2))

plot_confusion_matrix(train.matrix.logit2, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_logit2",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# logistic2 test 2x2 confusion matrix

test.matrix.logit2 <- as_tibble(table(ytest, fhat_test_logit2))

plot_confusion_matrix(test.matrix.logit2, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_logit2",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )
```


<br>

<br>

5. LASSO

```{r}
xtrain <- model.matrix(Diabetes_12 ~ . , data = train_set) #creating a matrix for the train set
xtest <- model.matrix(Diabetes_12 ~ . , data = test_set) #creating a matrix for the test set

#Takes a bit of time to run 
lasso_fit <- glmnet(xtrain, ytrain, alpha = 1, family = "multinomial")

# #Takes a lot of time to run.
# cv.out <- cv.glmnet(xtrain, ytrain, alpha = 1, family = "multinomial", nfolds = 5) #choosing the best lambda by cross validation. I chose K = 5 folds, because if i keep the default of K = 10, it takes FOREVER to run
# 
# bestlambda <- cv.out$lambda.min #outputting the best lambda

bestlambda <- 0.000464127


table_lasso <- as.data.frame(matrix(nrow = 9, ncol = 5))
colnames(table_lasso) <- c("Cutoff", "Accuracy", "Precision", "Recall", "Specificity")
table_lasso[,1] <- cutoffs

for (i in 1:length(cutoffs)){
   
   #training error
   phat_train_lasso <- as.data.frame(predict(lasso_fit, s = bestlambda, newx = xtrain, type = "response"))[,2]
   fhat_train_lasso <- ifelse(phat_train_lasso > cutoffs[i], 2, 1)
   train_error_lasso <- mean(fhat_train_lasso != ytrain)
   #train_error_logit 
   
   #test error
   phat_test_lasso <- as.data.frame(predict(lasso_fit, s = bestlambda, newx = xtest, type = "response"))[,2]
   fhat_test_lasso <- ifelse(phat_test_lasso > cutoffs[i], 2, 1)
   test_error_lasso <- mean(fhat_test_lasso != ytest)
   #test_error_logit
   
   lasso.cm <- confusionMatrix(as.factor(fhat_test_lasso), ytest, positive = "2")
   lasso.accuracy <- lasso.cm$overall[1]
   lasso.precision <- lasso.cm$byClass[3]
   lasso.recall <- lasso.cm$byClass[1]
   lasso.spec <- lasso.cm$byClass[2]
   
   table_lasso[i,2] <- lasso.accuracy
   table_lasso[i,3] <- lasso.precision
   table_lasso[i,4] <- lasso.recall 
   table_lasso[i,5] <- lasso.spec

   
}

table_lasso[,2:5] <- round(table_lasso[,2:5], digits = 2)
#table_lasso





```


```{r}
evaluate_on_binary_variables(fhat_test_lasso, ytest, test_set$Sex, "Sex")
evaluate_on_binary_variables(fhat_test_lasso, ytest, test_set$AnyHealthcare, "AnyHealthcare")
evaluate_on_income(fhat_test_lasso, ytest, test_set$Income)
evaluate_on_education(fhat_test_lasso, ytest, test_set$Education)
evaluate_on_age(fhat_test_lasso,  ytest, test_set$Age)
```

```{r}
lr_importance = (varImp(lasso_fit, lambda = bestlambda, scale = FALSE))
lr_importance <- cbind(newColName = rownames(lr_importance), lr_importance)
names(lr_importance) <- c("Feature", "Coefficient")
lr_importance <- lr_importance[order(lr_importance$Coefficient, decreasing = TRUE),]
lr_importance <- lr_importance[!lr_importance$Feature == "X.Intercept.",]
lr_importance <- lr_importance[!lr_importance$Feature == "X.Intercept..1",]

ggplot(lr_importance, aes(x = Coefficient, y = reorder(Feature, Coefficient)))+geom_col()+xlab("Maginitude of Coefficient") + ylab("Feature")
```


```{r}
# Lasso train 2x2 confusion matrix

train.matrix.lasso <- as_tibble(table(ytrain, fhat_train_lasso))

plot_confusion_matrix(train.matrix.lasso, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_lasso",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# Lasso test 2x2 confusion matrix

test.matrix.lasso <- as_tibble(table(ytest, fhat_test_lasso))

plot_confusion_matrix(test.matrix.lasso, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_lasso",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )
# 
# lasso.cm <- confusionMatrix(as.factor(fhat_test_lasso), ytest, positive = "2")
# lasso.accuracy <- lasso.cm$overall[1]
# lasso.precision <- lasso.cm$byClass[3]
# lasso.recall <- lasso.cm$byClass[1]
# lasso.spec <- lasso.cm$byClass[2]
```


BOOSTING

3. Boosting


```{r}
# #creating a vector of lambdas for the boosting model
# powers <- seq(-10, 0, by = 0.2)
# lambdas <- 10^powers
# 
# train_error_boost <- vector()
# test_error_boost <- vector()
# 
# #Fitting a boosting model for each lambda with 500 trees. I am using all the variables as predictors. I tried 1000 trees but it was taking so much time.
# 
# 
# 
# for (i in 1:length(lambdas)){
# 
#   #fitting boosting model
#    boost_fit <- gbm(as.character(as.factor(as.numeric(Diabetes_12) - 1))  ~ ., data = train_set, distribution = "bernoulli", n.trees = 500, shrinkage = lambdas[i])
# 
#    #training error
#    phat_train_boost <- predict(boost_fit, train_set, n.trees = 500, type = "response")#getting the probabilities of each class
#    fhat_train_boost <- ifelse(phat_train_boost > 0.5, 2, 1) #picking the class with the highest probability
#    train_error_boost[i] <- mean(fhat_train_boost != ytrain)
# 
#    #test error
#    phat_test_boost <- predict(boost_fit, test_set, n.trees = 500, type = "response") #getting the probabilities of each class
#    fhat_test_boost <- ifelse(phat_test_boost > 0.5, 2, 1) #picking the class with the highest probability
#    test_error_boost[i] <- mean(fhat_test_boost != ytest)
# 
#  }
# 
# #creating a data frame that has the train and test errors for each lambda
# Errors <- data.frame(Lambda = lambdas, trainError = train_error_boost, testError = test_error_boost)
```


```{r}

# #Plotting Training MSE vs lambda
# Errors %>% ggplot(aes(x = Lambda, y = trainError))+
#   geom_point(color = "cyan4", lwd = 1)+
#   labs(x = "Shrinkage Parameter", y = "Training Error", title = "Training MSE for Different Values of the Shrinkage Parameter")+
#   theme_bw()
# 
# #Plotting Test MSE vs lambda
# Errors %>% ggplot(aes(x = Lambda, y = testError))+
#   geom_point(color = "cyan4", lwd = 1)+
#   labs(x = "Shrinkage Parameter", y = "Test Error", title = "Test MSE for Different Values of the Shrinkage Parameter")+
#   theme_bw()
# 
```


```{r}
# 
# #finding which lambda leads to the lowest test error
# mintestError <- Errors$testError[which.min(Errors$testError)]
# mintestError
# minlambda <- Errors$Lambda[which.min(Errors$testError)]
# minlambda
```


```{r}
# Running boostin model with minlambda 

minlambda <- 0.02511886

boost_fit_best <- gbm(as.character(as.factor(as.numeric(Diabetes_12) - 1)) ~ . , data = train_set, distribution = "bernoulli", n.trees = 500, shrinkage = minlambda)


table_boost <- as.data.frame(matrix(nrow = 9, ncol = 5))
colnames(table_boost) <- c("Cutoff", "Accuracy", "Precision", "Recall", "Specificity")
table_boost[,1] <- cutoffs

for (i in 1:length(cutoffs)){
   
   #training error
   phat_train_boost <- predict(boost_fit_best, train_set, n.trees = 500, type = "response")#getting the probabilities of each class
   fhat_train_boost <- ifelse(phat_train_boost > cutoffs[i], 2, 1)#picking the class with the highest probability
   train_error_boost <- mean(fhat_train_boost != ytrain)
   train_error_boost
   
   #test error
   phat_test_boost <- predict(boost_fit_best, test_set, n.trees = 500, type = "response") #getting the probabilities of each class
   fhat_test_boost <- ifelse(phat_test_boost > cutoffs[i], 2, 1) #picking the class with the highest probability
   test_error_boost <- mean(fhat_test_boost != ytest)
   test_error_boost
   
   boost.cm <- confusionMatrix(as.factor(fhat_test_boost), ytest, positive = "2")
   boost.accuracy <- boost.cm$overall[1]
   boost.precision <- boost.cm$byClass[3]
   boost.recall <- boost.cm$byClass[1]
   boost.spec <- boost.cm$byClass[2]
   
   table_boost[i,2] <- boost.accuracy
   table_boost[i,3] <- boost.precision
   table_boost[i,4] <- boost.recall 
   table_boost[i,5] <- boost.spec

   
}

table_boost[,2:5] <- round(table_boost[,2:5], digits = 2)
#table_lasso

```


```{r}
evaluate_on_binary_variables(fhat_test_boost, ytest, test_set$Sex, "Sex")
evaluate_on_binary_variables(fhat_test_boost, ytest, test_set$AnyHealthcare, "AnyHealthcare")
evaluate_on_income(fhat_test_boost, ytest, test_set$Income)
evaluate_on_education(fhat_test_boost, ytest, test_set$Education)
evaluate_on_age(fhat_test_boost, ytest, test_set$Age)
```

```{r}
# Boost train 2x2 confusion matrix

train.matrix.boost <- as_tibble(table(ytrain, fhat_train_boost))

plot_confusion_matrix(train.matrix.boost, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_boost",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# Boost test 2x2 confusion matrix

test.matrix.boost <- as_tibble(table(ytest, fhat_test_boost))

plot_confusion_matrix(test.matrix.boost, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_boost",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )
```

```{r}
shap_values=predict(boost_fit_best, test_set, predcontrib = TRUE, approxcontrib = F)
```

```{r}

pred_fun <- function(boost_fit_best, newdata) {
  predict(boost_fit_best, newdata, n.trees = 500, type = "response")
}

shap <- explain(boost_fit_best, X = test_set[, -1], pred_wrapper = pred_fun, nsim = 51)
library(ggplot2)
autoplot(shap)

```


















```{r}
# Grabbing numbers from boosting html file
boosting.TP <- 1094
boosting.TN <- 53910
boosting.FP <- 772
boosting.FN <- 7644

# Metrics
boost.accuracy <- (boosting.TP + boosting.TN) / (boosting.TP + boosting.TN + boosting.FP + boosting.FN)
boost.precision <- boosting.TP / (boosting.TP + boosting.FP)
boost.recall <- boosting.TP / (boosting.TP + boosting.FN)
boost.spec <- boosting.TN / (boosting.TN + boosting.FP)
```


```{r}
# Metrics
nn.accuracy <- 0.8627
nn.precision <- 0.7019
nn.recall <- 0.0318
nn.spec <- 0.9978
```


```{r}
# Creating Metrics Table

table2 <- data.frame(matrix(ncol = 5, nrow = 6))
colnames(table2) <- c("Model", "Accuracy", "Precision", "Recall", "Specificity")

# LDA metrics
table2[1,] <- c("LDA", round(lda.accuracy, digits = 2), round(lda.precision, digits = 2), format(round(lda.recall, digits = 2), nsmall = 2), round(lda.spec, digits = 2))


# QDA metrics
table2[2,] <- c("QDA", round(qda.accuracy, digits = 2), round(qda.precision, digits = 2), round(qda.recall, digits = 2), format(round(qda.spec, digits = 2), nsmall = 2))


# Boosting metrics
table2[3,] <- c("Boosting", round(boost.accuracy, digits = 2), round(boost.precision, digits = 2), round(boost.recall, digits = 2), round(boost.spec, digits = 2))


# Logistic metrics
table2[4,] <- c("Logistic", round(logit.accuracy, digits = 2), round(logit.precision, digits = 2), round(logit.recall, digits = 2), round(logit.spec, digits = 2))


# Lasso metrics
table2[5,] <- c("Lasso", round(lasso.accuracy, digits = 2), round(lasso.precision, digits = 2), round(lasso.recall, digits = 2), round(lasso.spec, digits = 2))


# NN metrics
table2[6,] <- c("Neural Network", round(nn.accuracy, digits = 2), format(round(nn.precision, digits = 2), nsmall = 2), round(nn.recall, digits = 2), format(round(nn.spec, digits = 2), nsmall=2))

table2
```


```{r}
library(gt)

table2 %>%
  gt() %>%
  tab_header(
    title = md("**Overall Metrics for Various Models**"),
    subtitle = md("At the default cutoff of 0.5")
  )
```





















Tables For each model and for each cutoff
```{r}

All <- rbind( table_lasso,table_lda, table_qda, table_logit, table_boost)
All$Model <- c(rep("LASSO", 9), rep("LDA", 9), rep("QDA", 9), rep("Logistic", 9), rep("Boosting", 9))

All %>% gt(groupname_col = "Model")

```


