---
title: '263 Project: Redo with Two Classes'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(MASS)
library(gbm)
library(glmnet)
library(nnet)
library(cvms)
library(tibble)
library(table1)
library(caret)
library(ISLR)
```



```{r}

#Loading the dataset 

diabetes <- read.csv(file = "diabetes_012_health_indicators_BRFSS2015.csv", header = TRUE)
#View(diabetes)

# move anyone from class 0 into class 1
diabetes$Diabetes_012[diabetes$Diabetes_012 == 0] <- 1
colnames(diabetes)[1] <- "Diabetes_12"

diabetes$Diabetes_12 <- as.factor(diabetes$Diabetes_12)

# factor_variables <- c(1:4,6:15,18:22) #the columns that should be factors
# 
# #making the variables as factors
# for (i in factor_variables){
#   diabetes[,i] <- as.factor(diabetes[,i])
# }

```



Table 1:

```{r}

diabetes_table1 <- diabetes

diabetes_table1$Diabetes_12 <- factor(diabetes_table1$Diabetes_12, levels = c("1", "2"), labels = c("Non-Diabetic or Prediabetic", "Diabetic"))
label(diabetes_table1$Diabetes_12) <- "Diabetes Status"

diabetes_table1$HighBP <- factor(diabetes_table1$HighBP, levels = c("0","1"), labels = c("No High BP", "High BP"))
label(diabetes_table1$HighBP) <- "Blood Pressure"

diabetes_table1$HighChol <- factor(diabetes_table1$HighChol, levels = c("0", "1"), labels = c("No High Cholesterol", "High Cholesterol"))
label(diabetes_table1$HighChol) <- "Cholesterol Level"

diabetes_table1$CholCheck <- factor(diabetes_table1$CholCheck, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$CholCheck) <- "Cholesterol Check in past 5 Years"

diabetes_table1$Smoker <- factor(diabetes_table1$Smoker, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Smoker) <- "Smoked At Least 100 Cigs in Lifetime"

diabetes_table1$Stroke <- factor(diabetes_table1$Stroke, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Stroke) <- "Ever Told Had a Stroke"

diabetes_table1$HeartDiseaseorAttack <- factor(diabetes_table1$HeartDiseaseorAttack , levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$HeartDiseaseorAttack) <- "CHD or Myocardial Infarction"

diabetes_table1$PhysActivity <- factor(diabetes_table1$PhysActivity , levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$PhysActivity) <- "Physical Activity in Past 30 Days"

diabetes_table1$Fruits <- factor(diabetes_table1$Fruits, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Fruits) <- "Consume At Least 1 Fruit per Day"

diabetes_table1$Veggies <- factor(diabetes_table1$Veggies, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$Veggies) <- "Consume At Least 1 Vegetable per Day"

diabetes_table1$HvyAlcoholConsump <- factor(diabetes_table1$HvyAlcoholConsump, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$HvyAlcoholConsump) <- "Heavy Drinker"

diabetes_table1$AnyHealthcare <- factor(diabetes_table1$AnyHealthcare, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$AnyHealthcare) <- "Have Any Kind of Health Care Coverage"

diabetes_table1$NoDocbcCost <- factor(diabetes_table1$NoDocbcCost, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$NoDocbcCost) <- "Could Not See a Doctor in Past 12 Months Due To Cost"

diabetes_table1$GenHlth <- factor(diabetes_table1$GenHlth, levels = c("1", "2", "3", "4", "5"), labels = c("Excellent", "Very Good", "Good", "Fair", "Poor"))
label(diabetes_table1$GenHlth) <- "General Health Status"

label(diabetes_table1$MentHlth) <- "Nb Days Mental Health Not Good in Past 30 Days"

label(diabetes_table1$PhysHlth) <- "Nb Days Physical Health Not Good in Past 30 Days"

diabetes_table1$DiffWalk <- factor(diabetes_table1$DiffWalk, levels = c("0", "1"), labels = c("No", "Yes"))
label(diabetes_table1$DiffWalk) <- "Serious Difficulty Walking or Climbing Stairs"

diabetes_table1$Sex <- factor(diabetes_table1$Sex, levels = c("0", "1"), labels = c("Female", "Male"))
label(diabetes_table1$Sex) <- "Sex Assigned at Birth"

diabetes_table1$AgeBinary <- ifelse(diabetes_table1$Age < 9, "Young", "Old")
diabetes_table1$AgeBinary <- factor(diabetes_table1$AgeBinary, levels = c("Young", "Old"), labels = c("Less Than 60 Years Old", "At Least 60 Years Old"))
label(diabetes_table1$AgeBinary) <- "Age"

diabetes_table1$Education <- factor(diabetes_table1$Education, levels = c("1","2", "3", "4", "5", "6"), labels = c("Never Attended School", "Grades 1 Through 8 (Elementary)", "Grades 9 Through 11 (Some High School)", "Grade 12 or GED (High School Graduate)", "1 to 3 Years of College (Some College or Technical School)", " 4 or More Years of College (College Graduate)"))
label(diabetes_table1$Education) <- "Highest Education Level"

diabetes_table1$IncomeBinary <- ifelse(diabetes_table1$Income < 6, "Low", "High")
diabetes_table1$IncomeBinary <- factor(diabetes_table1$IncomeBinary, levels = c("Low", "High"), labels = c("At Most $35,000", "More Than $35,000"))
label(diabetes_table1$IncomeBinary) <- "Annual Income"


table1(~ Sex + AgeBinary + HighBP + HighChol + CholCheck + BMI + Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + Fruits + Veggies + HvyAlcoholConsump + AnyHealthcare + NoDocbcCost + GenHlth + MentHlth + PhysHlth + DiffWalk  + Education +  IncomeBinary | Diabetes_12, data = diabetes_table1, overall = "Total")


```

```{r}

set.seed(356) #Daniel's record

#splitting the dataset into train and test sets
n <- nrow(diabetes)
train_indices <- sample(1:n, size = 0.75*n, replace = F) #picking 3/4 of the dataset to be the training set and 25% to be the test set

train_set <- diabetes[train_indices, ] #training set
ytrain <- train_set$Diabetes_12

test_set <- diabetes[-train_indices, ] #test set
ytest <- test_set$Diabetes_12
```


<br>

<br>
```{r}
get_recall <- function(pred, target){
   TP = sum((pred==2) & (target==2))
   TN = sum((pred==1) & (target==1))
   FP = sum((pred==2) & (target==1))
   FN = sum((pred==1) & (target==2))
   recall = TP/(TP+FN)
   precision = TP/(TP+FP)
   accuracy = (TP+TN)/(TP+TN+FP+FN)
   recall
}

get_precision <- function(pred, target){
   TP = sum((pred==2) & (target==2))
   TN = sum((pred==1) & (target==1))
   FP = sum((pred==2) & (target==1))
   FN = sum((pred==1) & (target==2))
   accuracy = (TP+TN)/(TP+TN+FP+FN)
   accuracy
}

get_accuracy <- function(pred, target){
   TP = sum((pred==2) & (target==2))
   TN = sum((pred==1) & (target==1))
   FP = sum((pred==2) & (target==1))
   FN = sum((pred==1) & (target==2))
   accuracy = (TP+TN)/(TP+TN+FP+FN)
   accuracy
}
```


```{r}
evaluate_on_income <- function(pred, target, income) {
   df <- data.frame(pred, target, income)
   colnames(df) <- c('pred','target','income')
   list_metrices <- list()
   for (i in (1:8)){
      filtered_df = df[df$income==i, ]
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i]] = c(p, r, a, fraction)
   }
   list_metrices
}
```


```{r}
evaluate_on_education<- function(pred, target, education) {
   df <- data.frame(pred, target, education)
   colnames(df) <- c('pred','target','education')
   list_metrices <- list()
   for (i in (1:6)){
      filtered_df = df[df$education==i, ]
      print(dim(filtered_df)[1])
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i]] = c(p, r, a, fraction)
   }
   list_metrices
}
```

```{r}
evaluate_on_age<- function(pred, target, age) {
   df <- data.frame(pred, target, age)
   colnames(df) <- c('pred','target','age')
   list_metrices <- list()
   for (i in (1:13)){
      filtered_df = df[df$age==i, ]
      print(dim(filtered_df)[1])
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i]] = c(p, r, a, fraction)
   }
   list_metrices
}
```

```{r}
evaluate_on_binary_variables <- function(pred, target, variable_values, variable_name) {
   df <- data.frame(pred, target, variable_values)
   colnames(df) <- c('pred','target',variable_name)
   list_metrices <- list()
   for (i in (0:1)){
      filtered_df = df[df[,variable_name]==i, ]
      p = get_precision(filtered_df$pred, filtered_df$target)
      r = get_recall(filtered_df$pred, filtered_df$target)
      a = get_accuracy(filtered_df$pred, filtered_df$target)
      fraction = dim(filtered_df)[1]/dim(df)[1]
      list_metrices[[i+1]] = c(p, r, a, fraction)
   }
   list_metrices
}
```

1. LDA 

```{r}
#fitting LDA model using all the variables as predictors

lda_fit <- lda(Diabetes_12 ~ ., data = train_set, method = "mle")

#predictions using training set
fhat_train_lda <- predict(lda_fit, newdata = train_set, method = "plug-in")$class
train_error_lda <- mean(fhat_train_lda != ytrain) #training error
train_error_lda 

#predictions using test set
fhat_test_lda <- predict(lda_fit, newdata = test_set, method = "plug-in")$class
test_error_lda <- mean(fhat_test_lda != ytest) #test error
test_error_lda 
```


```{r}
# In the middle of each tile, we have the normalized count (overall percentage) and, beneath it, the count.
# At the bottom, we have the column percentage.
# At the right side of each tile, we have the row percentage. 


# LDA training 2x2 confusion matrix -- actual value, predicted value

train.matrix.lda <- as_tibble(table(ytrain, fhat_train_lda))

plot_confusion_matrix(train.matrix.lda, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_lda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# LDA Test 2x2 confusion matrix -- actual value, predicted value

test.matrix.lda <- as_tibble(table(ytest, fhat_test_lda))

plot_confusion_matrix(test.matrix.lda, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_lda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )

lda.cm <- confusionMatrix(fhat_test_lda, ytest, positive = "2")
lda.accuracy <- lda.cm$overall[1]
lda.precision <- lda.cm$byClass[3]
lda.recall <- lda.cm$byClass[1]
lda.spec <- lda.cm$byClass[2]
```






<br>

<br>


2. QDA

```{r}
#fitting QDA model using all the variables as predictors

qda_fit <- qda(Diabetes_12 ~ ., data = train_set, method = "mle")

#predictions using training set
fhat_train_qda <- predict(qda_fit, newdata = train_set, method = "plug-in")$class
train_error_qda <- mean(fhat_train_qda != ytrain)
train_error_qda 

#predictions using test set
fhat_test_qda <- predict(qda_fit, newdata = test_set, method = "plug-in")$class
test_error_qda <- mean(fhat_test_qda != ytest)
test_error_qda 
```

```{r}
# QDA train 2x2 confusion matrix

train.matrix.qda <- as_tibble(table(ytrain, fhat_train_qda))

plot_confusion_matrix(train.matrix.qda, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_qda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# QDA test 2x2 confusion matrix

test.matrix.qda <- as_tibble(table(ytest, fhat_test_qda))

plot_confusion_matrix(test.matrix.qda, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_qda",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )

qda.cm <- confusionMatrix(fhat_test_qda, ytest, positive = "2")
qda.accuracy <- qda.cm$overall[1]
qda.precision <- qda.cm$byClass[3]
qda.recall <- qda.cm$byClass[1]
qda.spec <- qda.cm$byClass[2]
```

interesting that QDA is correctly predicting more people who actually have diabetes (and is also falsely predicting too many people to have diabetes). this might actually be more useful


<br>

<br>


4. Logistic

```{r}
#Fitting a multinomial model with all variables as predictors.
logit_fit <- glm(Diabetes_12 ~ . , family = binomial(), data = train_set)

#training error
phat_train_logit <- predict(logit_fit, train_set, type = "response")
fhat_train_logit <- ifelse(phat_train_logit > 0.5, 2, 1)
train_error_logit <- mean(fhat_train_logit != ytrain)
train_error_logit 

#test error
phat_test_logit <- predict(logit_fit, test_set)
fhat_test_logit <- ifelse(phat_test_logit > 0.5, 2, 1)
test_error_logit <- mean(fhat_test_logit != ytest)
test_error_logit

```

can use a different cutoff for logistic!

```{r}
# logistic train 2x2 confusion matrix

train.matrix.logit <- as_tibble(table(ytrain, fhat_train_logit))

plot_confusion_matrix(train.matrix.logit, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_logit",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# logistic test 2x2 confusion matrix

test.matrix.logit <- as_tibble(table(ytest, fhat_test_logit))

plot_confusion_matrix(test.matrix.logit, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_logit",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )

logit.cm <- confusionMatrix(as.factor(fhat_test_logit), ytest, positive = "2")
logit.accuracy <- logit.cm$overall[1]
logit.precision <- logit.cm$byClass[3]
logit.recall <- logit.cm$byClass[1]
logit.spec <- logit.cm$byClass[2]
```



Don't run this it will take 1 hour (literally). I ran it and the backward selection only removed the fruits and veggies variables. So I ran another multinomial model excluding fruits and veggies.
```{r}
#multinomial using backward selection to pick the best variables

#start <- multinom(Diabetes_012 ~ ., data = train_set)
#backward_multinom <- step(start, direction = "backward")
```

```{r}

#logistic model excluding fruits and veggies variable
logit_fit2 <- glm(Diabetes_12 ~ HighBP + HighChol + CholCheck + BMI + Smoker + Stroke + HeartDiseaseorAttack + PhysActivity + HvyAlcoholConsump + AnyHealthcare + NoDocbcCost + GenHlth + MentHlth + PhysHlth + DiffWalk + Sex + Age + Education + Income, family = binomial(), data = train_set )

#training error
phat_train_logit2 <- predict(logit_fit2, train_set)
fhat_train_logit2 <- ifelse(phat_train_logit2 > 0.5, 2, 1)
train_error_logit2 <- mean(fhat_train_logit2 != ytrain)
train_error_logit2 

#test error
phat_test_logit2 <- predict(logit_fit2, test_set)
fhat_test_logit2 <- ifelse(phat_test_logit2 > 0.5, 2, 1)
test_error_logit2 <- mean(fhat_test_logit2 != ytest)
test_error_logit2
```


```{r}
# logistic2 train 2x2 confusion matrix

train.matrix.logit2 <- as_tibble(table(ytrain, fhat_train_logit2))

plot_confusion_matrix(train.matrix.logit2, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_logit2",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# logistic2 test 2x2 confusion matrix

test.matrix.logit2 <- as_tibble(table(ytest, fhat_test_logit2))

plot_confusion_matrix(test.matrix.logit2, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_logit2",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )
```


<br>

<br>

5. LASSO

```{r}
xtrain <- model.matrix(Diabetes_12 ~ . , data = train_set) #creating a matrix for the train set
xtest <- model.matrix(Diabetes_12 ~ . , data = test_set) #creating a matrix for the test set

#Takes a bit of time to run 
lasso_fit <- glmnet(xtrain, ytrain, alpha = 1, family = "multinomial")

#Takes a lot of time to run.
cv.out <- cv.glmnet(xtrain, ytrain, alpha = 1, family = "multinomial", nfolds = 5) #choosing the best lambda by cross validation. I chose K = 5 folds, because if i keep the default of K = 10, it takes FOREVER to run

bestlambda <- cv.out$lambda.min #outputting the best lambda
bestlambda # = 5.28e-05

#training error
fhat_train_lasso <- predict(lasso_fit, s = bestlambda, newx = xtrain, type = "class")
train_error_lasso <- mean(fhat_train_lasso != ytrain)
train_error_lasso

#test error
fhat_test_lasso <- predict(lasso_fit, s = bestlambda, newx = xtest, type = "class")
test_error_lasso <- mean(fhat_test_lasso != ytest)
test_error_lasso
```

```{r}
# Lasso train 2x2 confusion matrix

train.matrix.lasso <- as_tibble(table(ytrain, fhat_train_lasso))

plot_confusion_matrix(train.matrix.lasso, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_lasso",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# Lasso test 2x2 confusion matrix

test.matrix.lasso <- as_tibble(table(ytest, fhat_test_lasso))

plot_confusion_matrix(test.matrix.lasso, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_lasso",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )

lasso.cm <- confusionMatrix(as.factor(fhat_test_lasso), ytest, positive = "2")
lasso.accuracy <- lasso.cm$overall[1]
lasso.precision <- lasso.cm$byClass[3]
lasso.recall <- lasso.cm$byClass[1]
lasso.spec <- lasso.cm$byClass[2]
```


```{r}
# Grabbing numbers from boosting html file
boosting.TP <- 1094
boosting.TN <- 53910
boosting.FP <- 772
boosting.FN <- 7644

# Metrics
boost.accuracy <- (boosting.TP + boosting.TN) / (boosting.TP + boosting.TN + boosting.FP + boosting.FN)
boost.precision <- boosting.TP / (boosting.TP + boosting.FP)
boost.recall <- boosting.TP / (boosting.TP + boosting.FN)
boost.spec <- boosting.TN / (boosting.TN + boosting.FP)
```


```{r}
# Grabbing numbers from NN file
nn.TP <- 854
nn.TN <- 43038
nn.FP <- 601
nn.FN <- 6243

# Metrics
nn.accuracy <- (nn.TP + nn.TN) / (nn.TP + nn.TN + nn.FP + nn.FN)
nn.precision <- nn.TP / (nn.TP + nn.FP)
nn.recall <- nn.TP / (nn.TP + nn.FN)
nn.spec <- nn.TN / (nn.TN + nn.FP)
```


```{r}
# Creating Metrics Table

table2 <- data.frame(matrix(ncol = 5, nrow = 6))
colnames(table2) <- c("Model", "Accuracy", "Precision", "Recall", "Specificity")

# LDA metrics
table2[1,] <- c("LDA", round(lda.accuracy, digits = 2), round(lda.precision, digits = 2), format(round(lda.recall, digits = 2), nsmall = 2), round(lda.spec, digits = 2))


# QDA metrics
table2[2,] <- c("QDA", round(qda.accuracy, digits = 2), round(qda.precision, digits = 2), round(qda.recall, digits = 2), format(round(qda.spec, digits = 2), nsmall = 2))


# Boosting metrics
table2[3,] <- c("Boosting", round(boost.accuracy, digits = 2), round(boost.precision, digits = 2), round(boost.recall, digits = 2), round(boost.spec, digits = 2))


# Logistic metrics
table2[4,] <- c("Logistic", round(logit.accuracy, digits = 2), round(logit.precision, digits = 2), round(logit.recall, digits = 2), round(logit.spec, digits = 2))


# Lasso metrics
table2[5,] <- c("Lasso", round(lasso.accuracy, digits = 2), round(lasso.precision, digits = 2), round(lasso.recall, digits = 2), round(lasso.spec, digits = 2))


# NN metrics
table2[6,] <- c("Neural Network", round(nn.accuracy, digits = 2), round(nn.precision, digits = 2), round(nn.recall, digits = 2), round(nn.spec, digits = 2))



table2
```


```{r}
library(gt)

table2 %>%
  gt() %>%
  tab_header(
    title = md("**Overall Metrics for Various Models**")
  )
```

