---
title: "boosting 2 classes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(MASS)
library(gbm)
library(glmnet)
library(nnet)
library(cvms)
library(tibble)
library(table1)
library("shapper")
install_shap()
```



```{r}

#Loading the dataset 

diabetes <- read.csv(file = "diabetes_012_health_indicators_BRFSS2015.csv", header = TRUE)
#View(diabetes)

# move anyone from class 0 into class 1
diabetes$Diabetes_012[diabetes$Diabetes_012 == 0] <- 1
colnames(diabetes)[1] <- "Diabetes_12"

diabetes$Diabetes_12 <- as.factor(diabetes$Diabetes_12)

# factor_variables <- c(1:4,6:15,18:22) #the columns that should be factors
# 
# #making the variables as factors
# for (i in factor_variables){
#   diabetes[,i] <- as.factor(diabetes[,i])
# }

```

```{r}

set.seed(356) #Daniel's record

#splitting the dataset into train and test sets
n <- nrow(diabetes)
train_indices <- sample(1:n, size = 0.75*n, replace = F) #picking 3/4 of the dataset to be the training set and 25% to be the test set

train_set <- diabetes[train_indices, ] #training set
ytrain <- train_set$Diabetes_12

test_set <- diabetes[-train_indices, ] #test set
ytest <- test_set$Diabetes_12
```

3. Boosting


```{r}
# #creating a vector of lambdas for the boosting model
# powers <- seq(-10, 0, by = 0.2)
# lambdas <- 10^powers
# 
# train_error_boost <- vector()
# test_error_boost <- vector()
# 
# #Fitting a boosting model for each lambda with 500 trees. I am using all the variables as predictors. I tried 1000 trees but it was taking so much time.
# 
# 
# 
# for (i in 1:length(lambdas)){
# 
#   #fitting boosting model
#    boost_fit <- gbm(as.character(as.factor(as.numeric(Diabetes_12) - 1))  ~ ., data = train_set, distribution = "bernoulli", n.trees = 500, shrinkage = lambdas[i])
# 
#    #training error
#    phat_train_boost <- predict(boost_fit, train_set, n.trees = 500, type = "response")#getting the probabilities of each class
#    fhat_train_boost <- ifelse(phat_train_boost > 0.5, 2, 1) #picking the class with the highest probability
#    train_error_boost[i] <- mean(fhat_train_boost != ytrain)
# 
#    #test error
#    phat_test_boost <- predict(boost_fit, test_set, n.trees = 500, type = "response") #getting the probabilities of each class
#    fhat_test_boost <- ifelse(phat_test_boost > 0.5, 2, 1) #picking the class with the highest probability
#    test_error_boost[i] <- mean(fhat_test_boost != ytest)
# 
#  }
# 
# #creating a data frame that has the train and test errors for each lambda
# Errors <- data.frame(Lambda = lambdas, trainError = train_error_boost, testError = test_error_boost)
```


```{r}

# #Plotting Training MSE vs lambda
# Errors %>% ggplot(aes(x = Lambda, y = trainError))+
#   geom_point(color = "cyan4", lwd = 1)+
#   labs(x = "Shrinkage Parameter", y = "Training Error", title = "Training MSE for Different Values of the Shrinkage Parameter")+
#   theme_bw()
# 
# #Plotting Test MSE vs lambda
# Errors %>% ggplot(aes(x = Lambda, y = testError))+
#   geom_point(color = "cyan4", lwd = 1)+
#   labs(x = "Shrinkage Parameter", y = "Test Error", title = "Test MSE for Different Values of the Shrinkage Parameter")+
#   theme_bw()
# 
```


```{r}
# 
# #finding which lambda leads to the lowest test error
# mintestError <- Errors$testError[which.min(Errors$testError)]
# mintestError
# minlambda <- Errors$Lambda[which.min(Errors$testError)]
# minlambda
```


```{r}
# Running boostin model with minlambda 

minlambda <- 0.02511886

boost_fit_best <- gbm(as.character(as.factor(as.numeric(Diabetes_12) - 1)) ~ . , data = train_set, distribution = "bernoulli", n.trees = 500, shrinkage = minlambda)

#training error
phat_train_boost <- predict(boost_fit_best, train_set, n.trees = 500, type = "response")#getting the probabilities of each class
fhat_train_boost <- ifelse(phat_train_boost > 0.5, 2, 1)#picking the class with the highest probability
train_error_boost <- mean(fhat_train_boost != ytrain)
train_error_boost

#test error
phat_test_boost <- predict(boost_fit_best, test_set, n.trees = 500, type = "response") #getting the probabilities of each class
fhat_test_boost <- ifelse(phat_test_boost > 0.5, 2, 1) #picking the class with the highest probability
test_error_boost <- mean(fhat_test_boost != ytest)
test_error_boost
```
```{r}
evaluate_on_binary_variables(fhat_test_boost, ytest, test_set$Sex, "Sex")
evaluate_on_binary_variables(fhat_test_boost, ytest, test_set$AnyHealthcare, "AnyHealthcare")
evaluate_on_income(fhat_test_boost, ytest, test_set$Income)
evaluate_on_education(fhat_test_boost, ytest, test_set$Education)
evaluate_on_age(fhat_test_boost, ytest, test_set$Age)
```

```{r}
# Boost train 2x2 confusion matrix

train.matrix.boost <- as_tibble(table(ytrain, fhat_train_boost))

plot_confusion_matrix(train.matrix.boost, 
                      target_col = "ytrain", 
                      prediction_col = "fhat_train_boost",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )


# Boost test 2x2 confusion matrix

test.matrix.boost <- as_tibble(table(ytest, fhat_test_boost))

plot_confusion_matrix(test.matrix.boost, 
                      target_col = "ytest", 
                      prediction_col = "fhat_test_boost",
                      counts_col = "n",
                      palette = "Greens",
                      add_sums = TRUE,
                      sums_settings = sum_tile_settings(
                        palette = "Oranges",
                        label = "Total",
                        tc_tile_border_color = "black"
                        )
                      )
```

```{r}
shap_values=predict(boost_fit_best, test_set, predcontrib = TRUE, approxcontrib = F)
```

```{r}

pred_fun <- function(boost_fit_best, newdata) {
  predict(boost_fit_best, newdata, n.trees = 500, type = "response")
}

shap <- explain(boost_fit_best, X = test_set[, -1], pred_wrapper = pred_fun, nsim = 51)
library(ggplot2)
autoplot(shap)

```

